---
title: "regis_r_bootcamp"
author: "regis andreoli"
date: "1/31/2022"
output:
  word_document:
    toc: yes
    toc_depth: '4'
  html_document:
    toc: yes
    toc_float: no
    toc_depth: 4
    number_sections: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r}
knitr::opts_chunk$set(echo=TRUE,message=FALSE,error=FALSE,warning=FALSE)
```

# load library
```{r, message=FALSE}
library(knitr)
library(rstudioapi)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(caret)
library(GGally)
library(mgcv)
library(lubridate)
```

# Load Data
```{r}
setwd(dirname(getActiveDocumentContext()$path))
df_weather <- read.csv("./data/weather.csv",header=TRUE,sep =",",comment.char ="#")
df_plantA <- read.csv("./data/A.csv",header=TRUE,sep =",",comment.char ="#")
df_plantB <- read.csv("./data/B.csv",header=TRUE,sep =",",comment.char ="#")
df_plantC <- read.csv("./data/C.csv",header=TRUE,sep =",",comment.char ="#")
```

# Data preparation
```{r}
df_weather$local_time <- as.POSIXct(df_weather$local_time,tz="GMT",format="%Y-%m-%d %H:%M")
df_plantA$Timestamp <- as.POSIXct(df_plantA$Timestamp,tz="GMT",format="%Y-%m-%d %H:%M:%S")
df_plantB$Timestamp <- as.POSIXct(df_plantB$Timestamp,tz="GMT",format="%Y-%m-%d %H:%M:%S")
df_plantC$Timestamp <- as.POSIXct(df_plantC$Timestamp,tz="GMT",format="%Y-%m-%d %H:%M:%S")
```

# Predictive Modeling
This part aims to explore the energy production prediction capability in the data. For a first attempt, the data is taken with an hourly resolution. This lead to a data set with 8760 entries over the entire year.

```{r}
df_plantA_resample <- df_plantA %>%
  mutate(datetime = floor_date(Timestamp, "1 hour")) %>%
  group_by(datetime) %>%
  summarise(across(Generation_kW:Overall_Consumption_Calc_kW, sum))

str(df_plantA_resample)
```

## Visual analysis
This is a first visual analysis of the data. The Graph is Supported by a GAM smoother.
```{r message=FALSE}
ggplot(data = df_plantA_resample,
  mapping = aes(y = Generation_kW, x = datetime)) +
  geom_point(size = 1, color = "grey69") +
  geom_smooth(method = "gam", color = "cornflowerblue")
```

## Time Series Analysis - With auto.arima
Now, the data is converted into a time series object. As the data has an hourly resolution, the time interval is set to 24, which correspond in this case to a seasonality of one day. The model is trained with the first 250 days of the year.
```{r}
library(forecast)

ts_1 <- ts((df_plantA_resample$Generation_kW), deltat = 1/24)
train <- window(ts_1, start = 1, end = 250)
fit <- auto.arima(train)
```

### Model Prediction
Then, a prediction is done from day 250 to 365. The result can be seen in the following plot. Red is the prediction.
```{r, echo=FALSE}
fc <- predict(fit, n.ahead = 115*24)
plot(ts_1, lty=3, cex = 0.1)
lines(train, lwd=1)
lines(fc$pred, lwd=2, col="red", cex = 0.1)
```

Because the previous plot is not very helpful, another plot is computed, but this time with a zoom focused on the prediction area. As it is to see, the prediction has some variation in the first few days. Then, it converges to value which is a little bit higher than the mean of the data.
```{r}
fc <- predict(fit, n.ahead = 115*24)
plot(ts_1, lty=3, cex = 0.1, xlim=c(240, 270), ylim=c(25, 125))
lines(train, lwd=1)
lines(fc$pred, lwd=2, col="red", cex = 0.1)

mean(df_plantA_resample$Generation_kW[df_plantA_resample$Generation_kW > 0])
```

## Time Series Analysis - with Manipulated Data
For the next attempt, some artificial years are generated and added to the time series. This, to be able to capture the seasonal effect over the entire year. Also, the time resolution is set as daily. This is done in order to better focus on the seasonality over the entire year. This entire section is done with an experimental intention.

```{r, echo=FALSE}
df_plantA_resample_2 <- df_plantA %>%
  mutate(datetime = floor_date(Timestamp, "24 hour")) %>%
  group_by(datetime) %>%
  summarise(across(Generation_kW:Overall_Consumption_Calc_kW, sum))

ts_2 <- ts((df_plantA_resample_2$Generation_kW), start = c(2019), deltat = 1/365)
```

Plot of new time series with a resolution of one day, with a GAM smoother.
```{r}
ggplot(data = df_plantA_resample_2,
  mapping = aes(y = Generation_kW, x = datetime)) +
  geom_point(size = 1, color = "grey69") +
  geom_smooth(method = "gam", color = "cornflowerblue")
```

First, some artificial years are created by simply adding noise with the jitter function. Because of the occurrence of minus values in this procedure, minus values have to be corrected to zero. With the new data set a TS decomposition is plotted via the function stl(). Then, a new model is trained and a prediction is performed. Again, the prediction performance is rather poor.
```{r, echo=FALSE}
ts_artif_1 <- {jitter(ts_2, factor=500, amount = NULL)}
ts_artif_2 <- {jitter(ts_2, factor=500, amount = NULL)}
ts_artif_1[][ts_artif_1[] < 0] <- 0
ts_artif_2[][ts_artif_2[] < 0] <- 0
```

```{r}
ts_artificial <- ts(c(ts_2, ts_artif_1, ts_artif_2), start = c(2019), deltat = 1/365)
decomp<-stl(ts_artificial, s.window = 365)
str(ts_artificial)
plot(decomp)
```

Train model with the first two years of the time series. To train the model, the auto.arima function is made use of.
```{r, echo=FALSE}
ts_artificial_a <- ts(c(ts_2, ts_artif_1, ts_artif_2), start = c(2019), frequency = 365)

train_2 <- window(ts_artificial_a, start = c(2019,1), end = c(2020,365))
fit_2 <- auto.arima(train_2)
arima_prediciton <- predict(fit_2, n.ahead = 100)

plot(ts_artificial_a, cex = 0.1)
lines(train_2)
lines(arima_prediciton$pred, col = "red")
```

## Time Series Analysis - with Manipulated Data - new approach
The previous creation of the artificial years brought up some concerns regarding the quality of those artificial data. In this part, the focus is set to discover a method to generate artificial years with similar heterescadsticity, mean and distribution to the real data. The method is as follow: : use of GAM smoother as base for an artificial year, then adding of normally distributed, left screwed noise.

For that, a new data frame with indexed data is created. This is done to avoid the handling of date type.
```{r}
df_artif <- data.frame(time = (1:365), generation_kw = df_plantA_resample_2$Generation_kW)
```

Now, a GAM is computed from the new data frame.
```{r}
gam_model <- gam(generation_kw ~ s(time), data = df_artif)
```

In order to get out of the GAM a data set of one year with a daily entry, 365 prediction are computed within the entire year. The result is plotted to visually check the result. As shown below, the data frame plot is, at least qualitatively, very similar to the ggplot GAM smoother line. This result is taken as a base for the artificial years. 
```{r}
df_time <- data.frame(time = c(1:365))
gam_prediction <- predict(gam_model, newdata = df_time)
plot(gam_prediction, cex = 0.1, )
```

After the creation of a "base year", noise is added. In order to keep the heteroskedastic behavior of the real data, the noise is added as a multiplication of a random coefficient. This coefficient is first created as a left screwed, normally distributed random number. Then, it is normalized from 0 to 1. Last, those noise coefficient between 0 and 1 are again multiplied with an random coefficient, in order to level the mean of the data set. A quick look at the plot show a satisfying result.

Add noise to smoothed base year
```{r}
library(BBmisc)
library(fGarch)
set.seed(4)
random_coef <- rsnorm(365, mean = 1, sd = 1, xi = 0.1)
ran_coef_norm <- normalize(random_coef, method = "range", range = c(0, 1))
#ran_coef_norm <- ran_coef_norm * 1.3
ran_coef_norm_2 <- ran_coef_norm * runif(ran_coef_norm, min = 0.3, max = 1.6)
gam_yr_with_noise <- gam_prediction * ran_coef_norm_2

par(mfrow=c(2,2))

print("df_plantA_resample_2$Generation_kW")
mean(df_plantA_resample_2$Generation_kW)
median(df_plantA_resample_2$Generation_kW)
hist(df_plantA_resample_2$Generation_kW, breaks = 11)

print("")
print("gam_prediction")
mean(gam_prediction)
median(gam_prediction)
hist(gam_prediction, breaks = 11)

print("")
print("gam_yr_with_noise")
hist(ran_coef_norm_2)
mean(gam_yr_with_noise)
median(gam_yr_with_noise)
plot(gam_yr_with_noise, cex = 0.2)
hist(gam_yr_with_noise, breaks = 11)
```

While the artificial year visually looks satisfying, the mean generally is too low. It is not easy to change the "noise" parameters in such a way, that the variance and the mean of the artificial year gets near a similar value of the real data. Therefore, a function is create which aims to find the best parameter setting:

First, a list of parameter sets is created. This list is then used in a function that loops through the list and creates for each set a noised year. The noised year is then compared with the real year by its mean and variance. A threshold for mean and variance is set. If the thresholds are cumulatively fulfilled, the function saves the year data in a list, which is then returned at the the end of the function.

### Function To Get Artificial Year

Create list of parameter sets.
```{r}
library(BBmisc)
xi <- c(25:1)/30
min <- c(1:50)/100
max <- c(60:90)/40
u <- list()
  for (i in xi) {
    for (n in min) {
      for (q in max) {
        o <- c(i, n, q)
        u <- rbind(u,o)
      }
    }
  }
```

Function that loops through parameter list and saves every generated year that is similar to real data by mean and variance.
```{r}
get_artificial_years <- function(mean_real_yr, variance_real_yr, gam_prediction) {
  
  div_mean <- 0
  div_var <- 0
  i = 1
  treshold_1 <- TRUE
  treshold_2 <- TRUE
  df_sets <- list()
  m <- (length(u)/3 - 1)
  
  for (n in u) {
    if (i == m) {
      break
    }
    random_coef <- rsnorm(365, mean = 1, sd = 1, xi = as.numeric(u[i,][1]))
    ran_coef_norm <- normalize(random_coef, method = "range", range = c(0, 1))
    ran_coef_norm_2 <- ran_coef_norm * runif(ran_coef_norm, min = as.numeric(u[i,][2]), max = as.numeric(u[i,][3]))

    yr_with_noise <- gam_prediction * ran_coef_norm_2
    div_mean <- (mean_real_yr / mean(yr_with_noise))
    div_var <- (variance_real_yr / var(yr_with_noise))
       
    treshold_1 <- (div_mean < 1.025  && div_mean > 0.95)
    treshold_2 <- (div_var < 1.05 && div_var > 0.95)
    
    if (treshold_1 && treshold_2) {
      df_sets <- rbind(df_sets, yr_with_noise)
      print(i)
      print(div_mean)
      print(div_var)
    }
    if (i %in% (c(1:1000)*2500)){
      print(i)
    }
    
    i <- i + 1
  }
  df_sets
}
```

Call the function to compute about 40'000 simulated years and and save all ones that meet the threshold requirements. With set.seed() to 41, four years fulfill the thresholds.
```{r, echo=FALSE, message=FALSE}
mean_real <- mean(df_plantA_resample_2$Generation_kW)
var_real <- var(df_plantA_resample_2$Generation_kW)

set.seed(41)
noised_years <- get_artificial_years(mean_real, var_real, gam_prediction)
```

Plot and compare the results with the real data. As it is to see, the result is very satisfying, except some outliers with values over ~1800.
```{r}
df_artif <- t(noised_years[2,])
array_artif <- array(as.numeric(unlist(df_artif)))
print("Real")
mean(df_plantA_resample_2$Generation_kW)
var(df_plantA_resample_2$Generation_kW)
print("")
print("Artif")
mean(array_artif)
var(array_artif)
plot(array_artif, cex = .2)
par(mfrow=c(2,2))
hist(df_plantA_resample_2$Generation_kW)
hist(array_artif)
```
Pull down the few outliers which are over 1800. This is done for all the returned artificial years.
```{r}
noised_years[1,][noised_years[1,] > 1800] <- (as.numeric(noised_years[1,]) * 0.75)
df_artif_2 <- t(noised_years[1,])
array_artif_2 <- array(as.numeric(unlist(df_artif_2)))
par(mfrow=c(2,2))
plot(array_artif_2, cex = .2)
hist(array_artif_2)
```
```{r, echo=FALSE}
noised_years[2,][noised_years[2,] > 1800] <- (as.numeric(noised_years[2,]) * 0.75)
noised_years[3,][noised_years[3,] > 1800] <- (as.numeric(noised_years[3,]) * 0.75)
noised_years[4,][noised_years[1,] > 1800] <- (as.numeric(noised_years[4,]) * 0.75)
```
### Plot final Results Of Artificial Data
Create TS with artificial years. Then investigate with the stl() function. Each year differs now much more than the others. This result is considered as successful. This experimental excursion delivers an interesting result which could be now further used to explore predictive modelling. As for now, this will be the end of this part.
```{r}
ts_artificial_2 <- ts(c(noised_year[1,], noised_year[2,], noised_year[3,], noised_year[4,]), start = c(2019), deltat = 1/365)
decomp_2 <- stl(ts_artificial_2, s.window = 1/24, t.window = 365)
plot(decomp_2)
```













