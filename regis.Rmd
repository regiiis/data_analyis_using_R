---
title: "regis_r_bootcamp"
author: "regis andreoli"
date: "1/31/2022"
output:
  word_document:
    toc: yes
    toc_depth: '4'
  html_document:
    toc: yes
    toc_float: no
    toc_depth: 4
    number_sections: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r}
knitr::opts_chunk$set(echo=TRUE,message=FALSE,error=FALSE,warning=FALSE)
```

# load library
```{r, message=FALSE}
library(knitr)
library(rstudioapi)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(caret)
library(GGally)
library(mgcv)
library(lubridate)
library(BBmisc)
library(fGarch)
```

# Load Data
```{r}
setwd(dirname(getActiveDocumentContext()$path))
df_weather <- read.csv("./data/weather.csv",header=TRUE,sep =",",comment.char ="#")
df_plantA <- read.csv("./data/A.csv",header=TRUE,sep =",",comment.char ="#")
df_plantB <- read.csv("./data/B.csv",header=TRUE,sep =",",comment.char ="#")
df_plantC <- read.csv("./data/C.csv",header=TRUE,sep =",",comment.char ="#")
```

# Data preparation
```{r}
df_weather$local_time <- as.POSIXct(df_weather$local_time,tz="GMT",format="%Y-%m-%d %H:%M")
df_plantA$Timestamp <- as.POSIXct(df_plantA$Timestamp,tz="GMT",format="%Y-%m-%d %H:%M:%S")
df_plantB$Timestamp <- as.POSIXct(df_plantB$Timestamp,tz="GMT",format="%Y-%m-%d %H:%M:%S")
df_plantC$Timestamp <- as.POSIXct(df_plantC$Timestamp,tz="GMT",format="%Y-%m-%d %H:%M:%S")
```

# Predictive Modeling
This part aims to explore the energy production prediction potential in the data. For a first attempt, the data is taken in an hourly resolution. This lead to a data set with 8760 entries over one entire year.

```{r, echo=FALSE}
df_plantA_resample <- df_plantA %>%
  mutate(datetime = floor_date(Timestamp, "1 hour")) %>%
  group_by(datetime) %>%
  summarise(across(Generation_kW:Overall_Consumption_Calc_kW, sum))
```

## Visual analysis
This is a first visual analysis of the data. The Graph is Supported by a GAM smoother.
```{r message=FALSE}
ggplot(data = df_plantA_resample,
  mapping = aes(y = Generation_kW, x = datetime)) +
  geom_point(size = 1, color = "grey69") +
  geom_smooth(method = "gam", color = "cornflowerblue")
```

## Time Series Analysis - With auto.arima
The explored model for prediction is a time series model, called SARIMA. To be able to compute those types of models, the data set has first to be converted into a time series object. As the data has an hourly resolution, the time interval of the TS object is set to 24, which correspond, in this case, to the seasonality of one day. The model is then trained with the first 250 days of the year.
```{r}
library(forecast)

ts_1 <- ts((df_plantA_resample$Generation_kW), deltat = 1/24)
train <- window(ts_1, start = 1, end = 250)
fit <- auto.arima(train)
```

### Model Prediction
The trained model is then used for a prediction. The predicted days are from day 250 to 365. The result can be seen in the following plot. Red are the predicted data, black are the real data.

```{r, echo=FALSE}
fc <- predict(fit, n.ahead = 115*24)
plot(ts_1, lty=3, cex = 0.1)
lines(train, lwd=1)
lines(fc$pred, lwd=2, col="red", cex = 0.1)
```

As one can see above, prediction range of the plot is not very helpful. So, another plot is computed, but this time with a zoom set on the prediction area. As it is to see, the prediction has some variation for the first few days. Then, it converges to value which is a little bit higher than the mean of the data.
```{r}
fc <- predict(fit, n.ahead = 115*24)
plot(ts_1, lty=3, cex = 0.1, xlim=c(240, 270), ylim=c(25, 125))
lines(train, lwd=1)
lines(fc$pred, lwd=2, col="red", cex = 0.1)
```

## Time Series Analysis - with Manipulated Data
Because the previous results were not satisfying at all, a few changes to the data set are undertaken. First, the resolution is set to daily. Secondly, some artificial years are generated and added to the time series. This, in order to be able to capture the seasonal effect over the entire year. This entire part is done with an experimental intention and from curiosity.
With the resolution set to daily, the data set shrink to 365 entries.

```{r, echo=FALSE}
df_plantA_resample_2 <- df_plantA %>%
  mutate(datetime = floor_date(Timestamp, "24 hour")) %>%
  group_by(datetime) %>%
  summarise(across(Generation_kW:Overall_Consumption_Calc_kW, sum))

ts_2 <- ts((df_plantA_resample_2$Generation_kW), start = c(2019), deltat = 1/365)
```

Plot of new time series with a resolution of one day and a GAM smoother.
```{r}
ggplot(data = df_plantA_resample_2,
  mapping = aes(y = Generation_kW, x = datetime)) +
  geom_point(size = 1, color = "grey69") +
  geom_smooth(method = "gam", color = "cornflowerblue")
```

First, some artificial years were created by simply adding noise with the jitter() function. Because of the occurrence of minus values in this procedure, minus values have to be corrected to zero in a second process. With the new data set, a TS decomposition is plotted via the function stl(). Then, a new model is trained and a forecast is predicted. Again, the prediction performance is rather poor.
```{r, echo=FALSE}
ts_artif_1 <- {jitter(ts_2, factor=500, amount = NULL)}
ts_artif_2 <- {jitter(ts_2, factor=500, amount = NULL)}
ts_artif_1[][ts_artif_1[] < 0] <- 0
ts_artif_2[][ts_artif_2[] < 0] <- 0
```

```{r}
ts_artificial <- ts(c(ts_2, ts_artif_1, ts_artif_2), start = c(2019), deltat = 1/365)
decomp<-stl(ts_artificial, s.window = 365)
str(ts_artificial)
plot(decomp)
```

Train model with the first two years of the time series. To train the model, the auto.arima function is made use of.
```{r, echo=FALSE}
ts_artificial_a <- ts(c(ts_2, ts_artif_1, ts_artif_2), start = c(2019), frequency = 365)

train_2 <- window(ts_artificial_a, start = c(2019,1), end = c(2020,365))
fit_2 <- auto.arima(train_2)
arima_prediciton <- predict(fit_2, n.ahead = 100)

plot(ts_artificial_a, cex = 0.1)
lines(train_2)
lines(arima_prediciton$pred, col = "red")
```

## Time Series Analysis - with Manipulated Data - new approach
In the previous part, the creation of the artificial years brought up some concerns regarding the quality of those artificial data. In this part, the focus is set to discover a method to generate artificial data similar to the real data in terms of variance, mean and distribution of the data. The method is as follow: :
1. Use of GAM smoother as base for an artificial year
2. Add normally distributed, left screwed noise.

For that, a new data frame with indexed data is created. This is done to avoid the handling of date type.
```{r}
df_artif <- data.frame(time = (1:365), generation_kw = df_plantA_resample_2$Generation_kW)
```

Now, a GAM is computed from the new data frame.
```{r}
gam_model <- gam(generation_kw ~ s(time), data = df_artif)
```

In order to generate a data set out of the GAM, 365 prediction are computed within the entire year. The result is plotted to visually check the result. As shown below, the data frame plot is, at least qualitatively, very similar to the ggplot GAM smoother line. This result is taken as a base for the artificial years. 
```{r}
df_time <- data.frame(time = c(1:365))
gam_prediction <- predict(gam_model, newdata = df_time)
plot(gam_prediction, cex = 0.1, )
```

After the creation of a "base year", noise is added. In order to keep the heteroskedastic behavior of the real data, the noise is added as a multiplication by a random coefficient. This coefficient is first created as a left screwed, normally distributed random number. Then, it is normalized from 0 to 1. Last, those noise coefficient between 0 and 1 are again multiplied with an random coefficient in a predefined range (0.3 to 1.6), in order to level the mean of the data set. A quick look at the plot show a satisfying result.

Add noise to smoothed base year
```{r}
set.seed(4)
random_coef <- rsnorm(365, mean = 1, sd = 1, xi = 0.1)
ran_coef_norm <- normalize(random_coef, method = "range", range = c(0, 1))
ran_coef_norm_2 <- ran_coef_norm * runif(ran_coef_norm, min = 0.3, max = 1.6)
gam_yr_with_noise <- gam_prediction * ran_coef_norm_2
```

```{r, echo=FALSE}
par(mfrow=c(2,2))

print("Real Data")
paste("Mean:     ", mean(df_plantA_resample_2$Generation_kW))
paste("Variance: ", var(df_plantA_resample_2$Generation_kW))
hist(df_plantA_resample_2$Generation_kW, breaks = 11, main="Real Data")

print("GAM prediction")
paste("Mean:     ", mean(gam_prediction))
paste("Variance: ", var(gam_prediction))
hist(gam_prediction, breaks = 11, main="GAM prediction")

print("Artificial Data")
paste("Mean:     ", mean(gam_yr_with_noise))
paste("Variance: ", var(gam_yr_with_noise))
plot(gam_yr_with_noise, cex = 0.2, main="Artificial Data")
hist(gam_yr_with_noise, breaks = 11, main="Artificial Data")
```

While the artificial year visually looks satisfying, the mean generally is too low. It is not easy to change the "noise" parameters in such a way, that the variance and the mean of the artificial year gets near the similar value of the real data. Therefore, a function is create which aims to find the best parameter setting:

First, a list of parameter sets is created. This list is then used in a function that loops through the list and creates, for each set, a noised year. The noised year is then compared with the real year by its mean and variance. A threshold for mean and variance is set and if they are cumulatively fulfilled, the function saves the year data in a list. At the end of the loop, the list of years is returned by the function.

### Function To Get Artificial Year

Create list of parameter sets.
```{r}
library(BBmisc)
xi <- c(25:1)/30
min <- c(1:50)/100
max <- c(60:90)/40
u <- list()
  for (i in xi) {
    for (n in min) {
      for (q in max) {
        o <- c(i, n, q)
        u <- rbind(u,o)
      }
    }
  }
```

Following is the function that loops through parameter list.
```{r}
get_artificial_years <- function(mean_real_yr, variance_real_yr, gam_prediction) {
  
  div_mean <- 0
  div_var <- 0
  i = 1
  treshold_1 <- TRUE
  treshold_2 <- TRUE
  df_sets <- list()
  m <- (length(u)/3 - 1)
  
  for (n in u) {
    if (i == m) {
      break
    }
    ran_coef <- rsnorm(365, mean = 1, sd = 1, xi = as.numeric(u[i,][1]))
    ran_coef_norm <- normalize(ran_coef, method = "range", range = c(0, 1))
    ran_coef_norm_2 <- ran_coef_norm * runif(ran_coef_norm, min = as.numeric(u[i,][2]), max = as.numeric(u[i,][3]))

    yr_with_noise <- gam_prediction * ran_coef_norm_2
    div_mean <- (mean_real_yr / mean(yr_with_noise))
    div_var <- (variance_real_yr / var(yr_with_noise))
       
    treshold_1 <- (div_mean < 1.025  && div_mean > 0.95)
    treshold_2 <- (div_var < 1.05 && div_var > 0.95)
    
    if (treshold_1 && treshold_2) {
      df_sets <- rbind(df_sets, yr_with_noise)
      print(i)
      print(div_mean)
      print(div_var)
    }
    if (i %in% (c(1:10000)*5000)){
      paste("Year computed: ", i)
    }
    
    i <- i + 1
  }
  df_sets
}
```

Call the function to compute about 40'000 simulated years and and save all ones that meet the threshold requirements. With set.seed() to 41, four years fulfill the thresholds.
```{r, echo=FALSE, message=FALSE}
mean_real <- mean(df_plantA_resample_2$Generation_kW)
var_real <- var(df_plantA_resample_2$Generation_kW)

set.seed(41)
noised_years <- get_artificial_years(mean_real, var_real, gam_prediction)
```

Plot and compare the results with the real data. As it is to see, the results appear to be satisfying, excepting the few outliers, which are values over ~1800.
```{r, echo=FALSE}
df_artif <- t(noised_years[2,])
array_artif <- array(as.numeric(unlist(df_artif)))
print("Real Data")
paste("Mean:     ", mean(df_plantA_resample_2$Generation_kW))
paste("Variance: ", var(df_plantA_resample_2$Generation_kW))
print("Artificial Data")
paste("Mean:     ", mean(array_artif))
paste("Variance: ", var(array_artif))
```
As last manipulation, the few outliers over a value of 1800 are pulled down. This is done for all the returned artificial years.
```{r}
noised_years[1,][noised_years[1,] > 1800] <- (as.numeric(noised_years[1,]) * 0.75)
df_artif_2 <- t(noised_years[1,])
array_artif_2 <- array(as.numeric(unlist(df_artif_2)))
par(mfrow=c(2,2))
plot(array_artif_2, cex = .2, main="Plot Artificial Data")
hist(array_artif_2, main = "Histogram Artificial Data")
```
```{r, echo=FALSE}
noised_years[2,][noised_years[2,] > 1800] <- (as.numeric(noised_years[2,]) * 0.75)
noised_years[3,][noised_years[3,] > 1800] <- (as.numeric(noised_years[3,]) * 0.75)
noised_years[4,][noised_years[1,] > 1800] <- (as.numeric(noised_years[4,]) * 0.75)
```
### Plot final Results Of Artificial Data
Create TS with artificial years. Then investigate with the stl() function. Each year differs now much more than the others. This result is considered as successful. This experimental excursion delivers an interesting result which could be now further used to explore predictive modelling. As for now, this will be the end of this part.
```{r}
ts_artificial_2 <- ts(c(noised_years[1,], noised_years[2,], noised_years[3,], noised_years[4,]), start = c(2019), deltat = 1/365)
decomp_2 <- stl(ts_artificial_2, s.window = 1/24, t.window = 365)
plot(decomp_2)
```













